---
title: "SF Forecasting"
output:
  html_document:
    df_print: paged
---

```{r include=FALSE}
# Database Connect --------------------------------------------------------

## Install Packages

### Redshift

if(!require(DBI)) {
  install.packages("DBI")
  library(RPostgres)
}

if(!require(RPostgreSQL)) {
  install.packages("RPostgreSQL")
  library(RPostgreSQL)
}

if(!require(redshiftTools)) {
  install.packages("redshiftTools")
library(redshiftTools)
}
  
library("aws.s3")

### Forecast

if(!require(lubridate)) {
  install.packages("lubridate")
  library(lubridate)
}

if(!require(forecast)) {
  install.packages("forecast")
  library(forecast)
}

if(!require(xts)){
  install.packages("xts")
  library(xts)
}

if(!require(tseries)){
  install.packages("tseries")
  library(tseries)
}

### General

if(!require(tidyverse)){
  install.packages("tidyverse")
  library(tidyverse)
}

# Functions ---------------------------------------------------------------

create_test_train <- function(ts, h = 1){
  dates = as.Date(time(ts))
  min_date = min(dates)
  max_date = max(dates)
  list(holdout = tail(ts, h),
       train = ts(ts,
                  frequency = 12,
                  start = c(year(min_date),
                            month(min_date)),
                  end = c(year(max_date - months(h)),
                          month(max_date - months(h)))))
}


# Data Import -------------------------------------------------------------

# Load credentials for your own Redshift login
file <- paste("Z:\\User Output\\sf_odavies\\R WD\\R to Redshift\\my_credentials.csv")
my_credentials <- read.csv(file, header = TRUE, sep = ",")

# Load the credentials into variables.
port1 <- as.character(my_credentials[1,1])
host1 <- as.character(my_credentials[1,2])
user1 <- as.character(my_credentials[1,3])
password1 <- as.character(my_credentials[1,4])
dbname1 <- as.character(my_credentials[1,5])

# Establish a connection with Redshift
conn <- dbConnect(RPostgres::Postgres(), 
                        host=host1, 
                        port=port1, 
                        user=user1, 
                        password=password1, 
                        dbname=dbname1, 
                        sslmode='require')

## Type SQL Query - Sum of Sales per Month per Customer

query1 <- paste(
  "select
      left(cal_submit_date, 4) || '-' || substring(cal_submit_date, 6, 2) || '-28' as date,
      sum(fulfilled_sales) as sales
   from sf_analytics.prod_order
   where 
      cal_submit_date between dateadd(month, - 48, '2018-04-01') and '2018-03-31'
      and rp_person_id > 0
      and brand_code in ('PLUMBFIX', 'ELECTRICFX')
   group by 1;"
)

```

# Introduction

This report details time series analysis of sales within TRADE+ customer group.The main aim here is to create a forecast model that can extrapolate 15 months worth of sales. To achieve this aim, we'll be using the Autoregressive Integrated Moving Average (ARIMA) model. 

The ARIMA model has three parameters which we should consider. They are usually denoted as ARIMA(p, d, q) where

- p is the number of autoregressive terms.
- d is the number of differences needed for stationarity.
- q is the number of lagged forecast errors in the prediction equation.

These parameters will be explained in more detail as we explore the data and apply statistical testing.

# Exploratory Analysis

The data are first sourced from the Redpoint:

```{r}
### Grab data
dat <- dbGetQuery(conn, query1)

### Convert string and order by date

dat$date <- ymd(dat$date)

dat <- dat %>%
        arrange(date)

### Max and Min Date

min_date <- min(dat$date)
max_date <- max(dat$date)

### Check data
head(dat)
```

After we re-order the data by date and extract the minimum and maximum date for our forecasting analysis. We can then represent this series in R using  `ts()`:
```{r}
## Create time series
sf_tradeplus_ts <- ts(
                      data = dat$sales,
                      start = c(year(min_date), month(min_date)),
                      end = c(year(max_date), month(max_date)),
                      frequency = 12
                      )
  
  sf_tradeplus_ts
```

Notice the function requires both the start date and the end date (in `c(year, month)` format) and frequency which is always number of obvservations in a year.

To visualise the data we plot the time series:

```{r figure.align = "center"}
# Raw time-series plot - notice trend/seasonality/variance
plot.xts(as.xts(sf_tradeplus_ts/1E6),
         type = "o",
         ylab = "Sum of Sales [£m]",
         xlab = "Date",
         main = "Trade+ Sales Over Time")
```

We notice a positive trend with sales increasing over time. Seasonality (repeated periodic trends) is also apparent with characteristic dips across Dec-Feb and peaks across Oct-Nov. We can use time-series decompostion to examine these features using `stl()`  

```{r}
### plot stl
autoplot(stl(sf_tradeplus_ts/1E6,
             s.window = 'periodic')) + geom_point()
```

These four plots represent the original data, the trend, the seasonality and the remainder. These factors will be important to consider as we commonly assume in many time series techniques (especially ARIMA) that the time series is stationary. 

(ADD A BIT MORE)

A stationary time series is one whose statistical properties such as mean, variance and correlation are all constant over time (i.e. to create the stationary time series, we need to minimise the trend and seasonality variance). To achieve this we can apply differencing to our data. 

```{r fig.height = 8, figure.align = "center"}
# plotting parameters
par(mfrow=c(4, 1),
    oma = c(3, 2, 0, 0),
    mar = c(0, 0 , 0, 1),
    mgp = c(2, 1, 0),
    xpd = FALSE)


# Raw sales plot - notice trend/seasonality/variance
plot.xts(as.xts(sf_tradeplus_ts/1E6),
         type = "o",
         ylab = "Sum of Sales [£m]",
         xlab = "Date",
         main = "Original Data")


# Diff sales plot - notice increasing variance
plot.xts(as.xts(diff(sf_tradeplus_ts/1E6)),
         type = "o",
         ylab = "Sum of Sales [£m]",
         xlab = "Date",
         main = "Differenced Data")


# log sales plot - notice trend/seasonality/variance
plot.xts(as.xts(log10(sf_tradeplus_ts/1E6)),
         type = "o",
         ylab = "Sum of Sales [£m]",
         xlab = "Date",
         main = "Log10 of Data")


# Diff log sales plot - looks stationary
plot.xts(as.xts(diff(log10(sf_tradeplus_ts/1E6))),
         type = "o",
         ylab = "Sum of Sales [£m]",
         xlab = "Date",
         main = "Differenced Log10 of Data")

# add x-axis label
title(xlab = "Date",
      outer = TRUE, line = 0)


```

By differencing the data, we managed to create a time series with no clear sign of trend. Therefore we can depict that it looks stationary. We can also take logs as seen above on the bottom two figures which can also help to minimise variance.

# Augmented Dickey-Fuller Test (ADF)

We can test whether this time-series is stationary or not by performing the Augmented Dickey-Fuller Test (ADF). The ADF is a significance test where the alternative hypothesis is that the data are stationary. A significant p-value (less than 0.5) implies we can reject the null hypothesis that the data are not stationary. Below are the p-value results for running ADF on the raw, differenced, log-transformed and differenced log-transformed series.

```{r warning=FALSE}
# Testing Stationarity
adf_raw <-  adf.test(sf_tradeplus_ts,
                     alternative = "stationary")
                     adf_diff <- adf.test(diff(sf_tradeplus_ts),
                     alternative = "stationary")
                     adf_log <-  adf.test(log10(sf_tradeplus_ts),
                     alternative = "stationary")
                     adf_diff_log <- adf.test(diff(log10(sf_tradeplus_ts)),
                     alternative = "stationary")
                     
data.frame(row.names = "p_value",
           raw = adf_raw$p.value,
           diff_raw = adf_diff$p.value,
           log = adf_log$p.value,
           diff_log = adf_diff_log$p.value)

```

We have significant p-values for both the differenced and differenced log-transformed data. This implies that when we apply ARIMA, we will need to difference the data once and set d = 1. 

# Autocorrelation Factor (ACF) and Partial Autocorrelation Factor (PACF) plots

We now have learned that the original data needed to be differenced once to get a stationary time-series. Next we determine the other parameters by creating ACF and PACF plots

```{r fig.width = 8}
par(mfrow = c(1, 2))

acf(diff(sf_tradeplus_ts))
pacf(diff(sf_tradeplus_ts))
```

(NEED SOME TIME TO READ OVER ACF and PACF)

# Autoregressive Integrated Moving Average (ARIMA)

Fortunately there's a function called `auto.arima()` in R that does all of these checks above for us and automatically calculates the forecast. We will test this out on the 'holdout' set.

```{r}
# create test sample
samples <- create_test_train(sf_tradeplus_ts, 15)

# create arima model
fit <- auto.arima(samples$train)
fit
```

Note that `auto.arima()` used ARIMA(0,1,1) where p = 0, d = 1 and q = 1.

```{r}

# forecast predictions
pred <- forecast(fit, h = 15)

#plot forecast vs predictions
ts.plot(
        samples$train / 1E6,
        pred$mean / 1E6,
        samples$holdout / 1E6,
        lty = c(1, 3, 5),
        ylab = "Sum of Sales [£ M]",
        main = "Sales Forecast per Month"
  )
legend("topleft", c("Training", "Forecast", "Actual"), lty  = c(1, 3, 5))

# Look at accuracy
accuracy(f = pred$mean, 
         x = samples$holdout)

# Difference %
sum(pred$mean)/sum(samples$holdout) - 1 
  
```

Finally we'll look at residuals with boxplot:

```{r}
boxplot(pred$residuals/1E6, ylab = "Residuals [M]")
```



